#### 数据降维

##### 特征选择API

```python
from sklearn.feature_selection import VarianceThreshold

var = VarianceThreshold(threshold=1.0)

data = var.fit_transform([[1,2,3],[4,5,6],[7,8,9]])

print(data)

```

##### PCA

* 是一种分析，简化数据集的技术

* 目的：是数据维数压缩，尽可能降低原始数据的纬度（复杂度），损失少量信息

* 可以削减回归分析或者聚类分析中特征的数量

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=0.9)

data = pca.fit_transform([[1,2,3],[4,5,6],[7,8,9]])

print(data)

```

#### 数据特征预处理

##### 归一化API

```python
from sklearn.preprocessing import MinMaxScaler

mm = MinMaxScaler(feature_range(2, 3))

data = mm.fit_transform([[1,2,3],[4,5,6],[7,8,9]])

print(data)

```

##### 标准化公式

```python
x' = (x - mean) / 标准差
```

##### 方差

```python
var = (x1-mean)^2 + (x2-mean)^2 + ...... / 每个特征值的样本数
```

##### 标准差

```python
标准差 = 方差开根号
```

##### 特征化API

```python
from sklearn.preprocessing import StandardScaler

std = StandardScaler(feature_range(2, 3))

data = std.fit_transform([[1,2,3],[4,5,6],[7,8,9]])

print(data)

```

##### 缺失值API

```python
from sklearn.preprocessing import Imputer

im = Imputer(missing_values="NaN", strategy="mean", axis=0)

data = im.fit_transform([[1,2],[np.nan, 3],[7,8]])

print(data)

```

#### 特征文本抽取和中文问题

```python
import sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()

data = cv.fit_transform(["人生苦短","我爱python"])

print(cv.get_feature_names())

print(data.toarray())
```

##### jieba中文分词器

```python
pip install jieba
```

```python
import jieba

jieba.cut("我是一个程序员")
```

##### 字典特征数据抽取

```python
import sklearn.feature_extraction import DictVectorizer

dict = DictVectorizer()

data = dict.fit_transform([{"人生苦短"},{"我爱python"}])

print(cv.get_feature_names())

print(data.toarray())
```

#### tfidf分析问题

##### Tf 词的频率

##### idf 逆文档频率

```python
import sklearn.feature_extraction import TfidfVectorizer

tf = TfidfVectorizer()

data = tf.fit_transform([{"人生苦短"},{"我爱python"}])

print(tf.get_feature_names())

print(data.toarray())
```

#### 机器学习算法

##### 估计器

* 估计器是一类实现了算法的API

##### 用于分类的估计器

* sklearn.neighbors k近邻算法

* sklearn.naive_bayes 贝叶斯

* sklearn.linear_model.LogisticRegression 逻辑回归

* sklearn.tree 决策树和随机森林

##### 用于回归的估计器

* sklearn.linear_model.LinearRegression 线性回归

* sklearn.linear_model.Ridge 岭回归

##### 数据类型[离散型,连续型数据]

* 离散型在区间内不可分,连续型数据区间内可分

##### 机器学习算法分类[监督学习,非监督学习]

* 监督学习 [ k近邻算法, 贝叶斯, 决策树和随机森林, 逻辑回归, 神经网络, 线性回归, 岭回归]

* 非监督学习 [ k-means ]

#### k近邻算法

##### 如果一个样本在特征空间中k个最相似(即特征空间最临近)的样本中的大多数属于某一个类别,则其也属于该类别

#### 两个样本的距离公式,也叫欧式距离

```python
a(a1, a2, a3) b(b1, b2, b3)

(a1-b1)^2 + (a2-b2)^2 + (a3-b3)^2 开根号
```

#### k近邻预测用户签到位置

```python
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import pandas as pd

def knncls():
    """
    x,y 用户坐标
    time 用户行动的时间戳
    place_id 签到人数总数特征
    """
    data = pd.read_csv('文件路径')

    # print(data.head(10))

    # 缩小范围
    data = data.query("x > 1.0 & x < 1.25 & y > 2.5 & y < 2.75")

    # 处理时间戳到秒
    time_value = pd.to_datetime(data['time'], unit='s')

    # 返回时间字典格式
    time_value = pd.DatetimeIndex(time_value)

    # print(time_value)

    data['day'] = time_value.day
    data['hour'] = time_value.hour
    data['weekday'] = time_value.weekday

    # 把时间戳删除, 1表示列
    data = data.drop(['time'], axis=1)

    # 处理签到位置
    place_count = data.groupby('place_id').count()

    tf = place_count[place_count.row_id > 3].reset_index()

    data = data[data['place_id'].isin(tf.place_id)]

    # 取出数据的特征值和目标值
    y = data['place_id'] # 目标值

    data = data.drop(['place_id'], axis=1)

    # 进行数据的分割训练集合测试集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

    # 标准化
    std = StandardScaler()

    # 对测试集和训练集的特征值进行标准化
    x_train= std.fit_transform(x_train)

    x_test = std.transform(x_test)

    # 算法
    knn = KNeighborsClassifier(n_neighbors=5)

    knn.fit(x_train, y_train)

    # 得出预测结果
    y_predict = knn.predict(x_test)

    print("预测的签到位置:", y_predict)

    # 得出准确率
    print("预测准确率:", knn.score(x_test, y_test))

if __name__ == "__main__":
    knncls()
```

#### 缺点

* k值很小: 容易受异常点影响

* k值很大: 容易受样本数量的波动

* 性能问题: 懒惰算法,对测试样本计算量大,内存开销大

#### 优点

* 简单,易于理解,易于实现,无需估计参数,无需训练


